{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9bdaa1f",
   "metadata": {
    "id": "b9bdaa1f"
   },
   "source": [
    "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float:right;\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c621fd",
   "metadata": {
    "id": "25c621fd"
   },
   "source": [
    "# 12th exercise: <font color=\"#C70039\">First Reinforcement Learning Game (*Frozen Lake*) using OpenAI Gym</font>\n",
    "* Course: AML\n",
    "* Lecturer: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Author of notebook: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>. This notebook is based on the great post and notebook from [Rodolfo Mendes](https://morioh.com/p/18a96b9091d3).\n",
    "* Student: Nicolas Rehbach\n",
    "* Matriculation Number: 11133387\n",
    "* Date:   19.12.2022\n",
    "\n",
    "<img src=\"https://launchyourintelligentapphome.files.wordpress.com/2019/05/frozenlake_legended.png?w=531\" style=\"float: center;\" width=\"600\">\n",
    "\n",
    "---------------------------------\n",
    "**GENERAL NOTE 1**: \n",
    "Please make sure you are reading the entire notebook, since it contains a lot of information on your tasks (e.g. regarding the set of certain paramaters or a specific computational trick), and the written mark downs as well as comments contain a lot of information on how things work together as a whole. \n",
    "\n",
    "**GENERAL NOTE 2**: \n",
    "* Please, when commenting source code, just use English language only. \n",
    "* When describing an observation please use English language, too.\n",
    "* This applies to all exercises throughout this course.\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"ce33ff\">DESCRIPTION</font>:\n",
    "\n",
    "#### OpenAI Gym\n",
    "In this exercise you will be using Python and OpenAI Gym to develop your reinforcement learning algorithm. The Gym library is a collection of environments that can be used freely with the reinforcement learning algorithms.\n",
    "\n",
    "Gym has a ton of environments ranging from simple text based games to Atari games like Breakout and Space Invaders. The library is intuitive to use and simple to install. Just run **pip install gym** and you are good to go! The link to Gym's installation instructions, requirements, and documentation is included in the description. \n",
    "\n",
    "Further reading about OpenAI Gym is available under https://www.gymlibrary.dev/.\n",
    "This notebook is based on this great post and notebook from [Rodolfo Mendes](https://morioh.com/p/18a96b9091d3).\n",
    "\n",
    "#### Frozen Lake\n",
    "This description of the game is copied directly from Gym's website. \n",
    "\n",
    "*Winter is coming. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water and die (Game over). At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. The surface is described using a grid like the following:*\n",
    "\n",
    "* SFFF\n",
    "* FHFH\n",
    "* FFFH\n",
    "* HFFG\n",
    "\n",
    "This grid is your environment! S is your (the agent's) starting point and it's safe. F represents the frozen surface and is also safe. H represents a hole and if your agent steps in a hole in the middle of a frozen lake, the game is over because the agent dies. Finally, G represents the goal, which is the space on the grid where the frisbee is located.\n",
    "\n",
    "The agent can navigate *left, right, up, down* and the episode ends when the agent reaches the goal or falls in a hole. It receives a reward of **1** if it reaches the goal and **0** otherwise.\n",
    "\n",
    "Here is the summary:\n",
    "<img src=\"https://github.com/gheisenberg/AML/blob/main/images/FrozenLake.States.Rewards.png?raw=1\" style=\"float: center;\" width=\"800\">\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"FFC300\">TASKS</font>:\n",
    "The tasks that you need to work on within this notebook are always indicated below as bullet points. \n",
    "If a task is more challenging and consists of several steps, this is indicated as well. \n",
    "Make sure you have worked down the task list and commented your doings. \n",
    "This should be done by using markdown.<br> \n",
    "<font color=red>Make sure you don't forget to specify your name and your matriculation number in the notebook.</font>\n",
    "\n",
    "**YOUR TASKS in this exercise are as follows**:\n",
    "1. import the notebook to Google Colab or use your local machine.\n",
    "2. make sure you specified you name and your matriculation number in the header below my name and date. \n",
    "    * set the date too and remove mine.\n",
    "3. read the entire notebook carefully \n",
    "    * add comments whereever you feel it necessary for better understanding\n",
    "    * run the notebook for the first time. \n",
    "4. install gym into your env!\n",
    "5. You will train an agent to play the *Frozen Lake* game using Q-learning and you will get a playback of how the agent does after being trained.\n",
    "6. Again the task: Your agent has to navigate the grid by staying on the frozen surface without falling into any holes until it reaches the frisbee. If it reaches the frisbee, it wins with a reward of plus one. If it falls in a hole, it loses and receives no points for the entire episode.\n",
    "7. Your tasks are highlighted in the notebook (see below)\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e1838",
   "metadata": {
    "id": "0c8e1838"
   },
   "source": [
    "### Imports \n",
    "import all important libs including gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8a696b3",
   "metadata": {
    "id": "a8a696b3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from   IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87efa371",
   "metadata": {
    "id": "87efa371"
   },
   "source": [
    "### Creating the Environment\n",
    "For creating your environment, just call *gym.make()* and pass a string of the name of the environment you want to set up. \n",
    "All the environments with their corresponding names you can use here are available on Gym's website (see above).\n",
    "With this *env* object, you are able to query for information about the environment, sample states and actions, retrieve rewards and have your agent navigate the frozen lake. That is all made available to you conveniently with Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88423b6",
   "metadata": {
    "id": "d88423b6"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TLOpIW3dB1pM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLOpIW3dB1pM",
    "outputId": "ef34e96c-cd0f-4a0e-cf0e-8e1b510c9bea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3fd99",
   "metadata": {
    "id": "9ba3fd99"
   },
   "source": [
    "### Creating the Q-Table\n",
    "Now, construct your Q-table, and initialize all the Q-values to zero for each state-action pair.\n",
    "The number of rows in the table is equivalent to the size of the state space in the environment, and the number of columns is equivalent to the size of the action space (see above). You can get this information using *env.observation_space.n* and *env.action_space.n* as shown below in the code. Then, you can use this information to build the Q-table and initialize it with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349627df",
   "metadata": {
    "id": "349627df"
   },
   "outputs": [],
   "source": [
    "# Creating our q table which consists of the \n",
    "# action space (up, down, right, left)\n",
    "action_space_size = env.action_space.n\n",
    "# state space (our 16 possible fields (4x4))\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac22125",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ac22125",
    "outputId": "ae276298-5d2d-4547-d57f-d5f29ee80427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Since our agent does not know anything, every state action pair is 0\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d662212",
   "metadata": {
    "id": "6d662212"
   },
   "source": [
    "### Initializing Q-Learning hyperparameters\n",
    "Now, we're going to create and initialize all the parameters needed to implement the Q-learning algorithm.\n",
    "\n",
    "First, with *num_episodes*, you define the total number of episodes you want the agent to play during training. Then, with *max_steps_per_episode*, you define a maximum number of steps that your agent is allowed to take within a single episode. So, if by the 100th step, the agent has not reached the frisbee or fallen through a hole, then the episode will terminate with the agent receiving zero points.\n",
    "\n",
    "Next, you will set your *learning_rate* and your *discount_rate* as well, which was represented with the symbol (lambda) in the course slides (keyword: discounted return G_t).\n",
    "\n",
    "Now, the last four parameters are all related to the exploration-exploitation dilemma with respect to the epsilon-greedy policy. You are initializing your *exploration_rate* to **1** and setting the *max_exploration_rate* to **1** and a *min_exploration_rate* to **0.01**. The *max* and *min* are just bounds to how large or small your exploration rate can be. Remember, the exploration rate was represented with the symbol (epsilon) when discussed in the course slides.\n",
    "\n",
    "Lastly, you will set the *exploration_decay_rate* to **0.01** to determine the rate at which the *exploration_rate* will decay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f34d94",
   "metadata": {
    "id": "40f34d94"
   },
   "source": [
    "**YOUR <font color=\"FFC300\">TASK</font> in this exercise is as follows** (point 7 from the task list above):\n",
    "\n",
    "All of the above parameters can change!\n",
    "Your task is to create a *testplan* and tune all parameters by yourself and observe how they influence and change the performance of the algorithm. \n",
    "Make notes! They will help you during the exam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dde16e1",
   "metadata": {
    "id": "3dde16e1"
   },
   "outputs": [],
   "source": [
    "num_episodes = 10000  # total number of episodes played during our training\n",
    "max_steps_per_episode = 100 # maximum steps the agent takes while episode\n",
    "\n",
    "learning_rate = 0.2 # learning rate epsilon\n",
    "discount_rate = 0.99 # discount rate \n",
    "\n",
    "exploration_rate = 1 \n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.001\n",
    "exploration_decay_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac087582",
   "metadata": {
    "id": "ac087582"
   },
   "source": [
    "Create a list to hold all of the rewards you will get from each episode. \n",
    "By means of this you can observe how your game score changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf6194e",
   "metadata": {
    "id": "7bf6194e"
   },
   "outputs": [],
   "source": [
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39502e30",
   "metadata": {
    "id": "39502e30"
   },
   "source": [
    "In the following code section, the entire Q-learning algorithm is implemented as discussed in detail in the AML course. \n",
    "When this code is executed, this is exactly where the training will take place. \n",
    "* The first for-loop contains everything that happens within a single episode. \n",
    "* The second nested loop contains everything that happens for a single time-step.\n",
    "\n",
    "Read all the red comments, as they contain lots of important information on the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DMwJNwFd8Avm",
   "metadata": {
    "id": "DMwJNwFd8Avm"
   },
   "source": [
    "1st Loop:\n",
    "\n",
    "For each episode, we reset the environment and adjust the reward.\n",
    "\n",
    "2nd Loop:\n",
    "- Our exploration rate threshhold is a random number from the uniform distribution\n",
    "- if the exploration rate threshhold is larger than our exploration rate our agent will exploit the environmnt\n",
    "- else our action is a random choice\n",
    "\n",
    "Take new action:\n",
    "- We call the step function which transitions us to a new state, reward, done and info\n",
    "\n",
    "Update Q-table based on the Bellman equation\n",
    "\n",
    "Set our current state to the new state and add the reward\n",
    "\n",
    "Adjust the exploration rate, which is the min_exploraton rate defined by us - the max_exploration rate - min_exploration rate multiplied times - our exploration decay rate * the number of episodes\n",
    "\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dad9e9",
   "metadata": {
    "id": "c6dad9e9"
   },
   "outputs": [],
   "source": [
    "# Q-learning algorithm\n",
    "\n",
    "# loop: for a single episode\n",
    "for episode in range(num_episodes):\n",
    "    # initialize 'new episode' parameters\n",
    "    state = env.reset()\n",
    "    ''' The done variable just keeps track of whether or not your episode is finished.\n",
    "    Initialize it to False when first starting the episode and you will see later where \n",
    "    it will get updated to notify you when the episode is over.'''\n",
    "    done = False\n",
    "    \n",
    "    ''' Keep track of the rewards within the current episode as well.\n",
    "    Hence, set rewards_current_episode = 0 since you start \n",
    "    with no rewards at the beginning of each episode.'''\n",
    "    rewards_current_episode = 0\n",
    "\n",
    "    # nested loop: for a single time-step\n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        '''For each time-step within an episode set your exploration_rate_threshold \n",
    "        to a random number between 0 and 1. This will be used to determine whether \n",
    "        your agent will explore or exploit the environment in this time-step.'''\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take new action\n",
    "        '''After action is chosen, take that action by calling step() on your env object and \n",
    "        pass your action to it. The function step() returns a tuple containing the new state, \n",
    "        the reward for the action you took, whether or not the action ended the episode and \n",
    "        diagnostic information regarding the environment (helpful for debugging).'''\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-table for Q(s,a)\n",
    "        '''Compare this implementation with the equation in the course slides.'''\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "        learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        '''Set your current state to the new_state that was returned when taking the last action \n",
    "        and then update the rewards from your current episode by adding the reward you received \n",
    "        for your previous action.'''\n",
    "        # Set new state\n",
    "        state = new_state\n",
    "        # Add new reward \n",
    "        rewards_current_episode += reward \n",
    "        '''Then, check to see if your last action ended the episode \n",
    "        (game over by agent stepping in a hole or reaching the goal)! \n",
    "        If the action did end the episode, then jump out of this loop and start the next episode.\n",
    "        Otherwise, transition to the next time-step.'''\n",
    "        if done == True: \n",
    "            break\n",
    "            \n",
    "\n",
    "    # Exploration rate decay\n",
    "    '''Once an episode is finished, you need to update your exploration_rate using exponential decay, \n",
    "    which just means that the exploration rate decays at a rate proportional to its current value. \n",
    "    You can decay the exploration_rate using the formula above, which makes use of all the exploration \n",
    "    rate parameters that were defined above in the hyperparameter section.'''\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    # Add current episode reward to total rewards list and move on to the next episode\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3acbce7",
   "metadata": {
    "id": "f3acbce7"
   },
   "source": [
    "### All episodes training completed\n",
    "After all episodes are finished you now just calculate the average reward per thousand episodes from your list that contains the rewards for all episodes so that you can print it out and see how the rewards changed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03097b41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03097b41",
    "outputId": "55a594df-71a7-4f0f-ddf0-f04d6813e165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.11500000000000009\n",
      "2000 :  0.16800000000000012\n",
      "3000 :  0.17200000000000013\n",
      "4000 :  0.2450000000000002\n",
      "5000 :  0.19200000000000014\n",
      "6000 :  0.17600000000000013\n",
      "7000 :  0.17400000000000013\n",
      "8000 :  0.19300000000000014\n",
      "9000 :  0.18000000000000013\n",
      "10000 :  0.21500000000000016\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e875b",
   "metadata": {
    "id": "d66e875b"
   },
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771b72f",
   "metadata": {
    "id": "6771b72f"
   },
   "source": [
    "From this print, you can see that the average reward per thousand episodes did indeed progress over time. When the algorithm first started training, the first thousand episodes only averaged a reward of almost **0.18**, but by the time it got to its last thousand episodes, the reward drastically improved to almost **0.7**.\n",
    "\n",
    "Let's take a second to understand how you can interpret these results. Your agent played **10000** episodes. At each time step within an episode, the agent received a reward of **1** if it reached the frisbee, otherwise, it received a reward of **0**. If the agent did indeed reach the frisbee, then the episode finished at that time-step.\n",
    "\n",
    "Hence, that means for each episode, the total reward received by the agent for the entire episode is either **1** or **0**. So, for the first thousand episodes, you can interpret this score as meaning that **18%** of the time the agent received a reward of **1** and won the episode. And by the last thousand episodes from a total of **10000**, the agent was winning almost **70%** of the time.\n",
    "\n",
    "By analyzing the grid of the game, you can see it is a lot more likely that the agent would fall in a hole or perhaps reach the max time steps than it is to reach the frisbee, so reaching the frisbee **70%** of the time is not too bad, especially since the agent had no explicit instructions to reach the frisbee. It learned that this is the correct thing to do.\n",
    "\n",
    "* SFFF\n",
    "* FHFH\n",
    "* FFFH\n",
    "* HFFG\n",
    "\n",
    "At last, print out your updated Q-table to see how that has transitioned from its initial state of all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fbc5bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68fbc5bc",
    "outputId": "48f67202-406e-478b-82fe-e3d54cd4eacb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "********Q-table********\n",
      "\n",
      "[[0.11658061 0.0554133  0.10225941 0.10106414]\n",
      " [0.02070561 0.02815919 0.02063497 0.07455307]\n",
      " [0.04840272 0.01470083 0.01657251 0.01755061]\n",
      " [0.01243346 0.01239125 0.00749147 0.03108224]\n",
      " [0.14382438 0.0395896  0.02108416 0.03687206]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.21830464 0.02810786 0.00338817 0.00344378]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.13257464 0.10605136 0.13856745 0.42008916]\n",
      " [0.28198252 0.51886139 0.10327252 0.21660502]\n",
      " [0.66192755 0.03303607 0.04343606 0.01257658]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.12461402 0.09829155 0.67990866 0.10151444]\n",
      " [0.32530198 0.97067467 0.49908067 0.21608362]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-xRwsFEFkMGc",
   "metadata": {
    "id": "-xRwsFEFkMGc"
   },
   "source": [
    "## Experimenting with different Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8qCbV7SGi4EU",
   "metadata": {
    "id": "8qCbV7SGi4EU"
   },
   "outputs": [],
   "source": [
    "def run_experiment(num_episodes, max_steps_per_episode, learning_rate, discount_rate, exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate):\n",
    "  env = gym.make(\"FrozenLake-v1\")\n",
    "  env.reset()\n",
    "\n",
    "  action_space_size = env.action_space.n\n",
    "  # state space (our 16 possible fields (4x4))\n",
    "  state_space_size = env.observation_space.n\n",
    "\n",
    "  q_table = np.zeros((state_space_size, action_space_size))\n",
    "  \n",
    "  rewards_all_episodes = []\n",
    "\n",
    "  # Q-learning algorithm\n",
    "\n",
    "  # loop: for a single episode\n",
    "  for episode in range(num_episodes):\n",
    "    # initialize 'new episode' parameters\n",
    "    state = env.reset()\n",
    "    ''' The done variable just keeps track of whether or not your episode is finished.\n",
    "    Initialize it to False when first starting the episode and you will see later where \n",
    "    it will get updated to notify you when the episode is over.'''\n",
    "    done = False\n",
    "    \n",
    "    ''' Keep track of the rewards within the current episode as well.\n",
    "    Hence, set rewards_current_episode = 0 since you start \n",
    "    with no rewards at the beginning of each episode.'''\n",
    "    rewards_current_episode = 0\n",
    "\n",
    "    # nested loop: for a single time-step\n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        '''For each time-step within an episode set your exploration_rate_threshold \n",
    "        to a random number between 0 and 1. This will be used to determine whether \n",
    "        your agent will explore or exploit the environment in this time-step.'''\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take new action\n",
    "        '''After action is chosen, take that action by calling step() on your env object and \n",
    "        pass your action to it. The function step() returns a tuple containing the new state, \n",
    "        the reward for the action you took, whether or not the action ended the episode and \n",
    "        diagnostic information regarding the environment (helpful for debugging).'''\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-table for Q(s,a)\n",
    "        '''Compare this implementation with the equation in the course slides.'''\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "        learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        '''Set your current state to the new_state that was returned when taking the last action \n",
    "        and then update the rewards from your current episode by adding the reward you received \n",
    "        for your previous action.'''\n",
    "        # Set new state\n",
    "        state = new_state\n",
    "        # Add new reward \n",
    "        rewards_current_episode += reward \n",
    "        '''Then, check to see if your last action ended the episode \n",
    "        (game over by agent stepping in a hole or reaching the goal)! \n",
    "        If the action did end the episode, then jump out of this loop and start the next episode.\n",
    "        Otherwise, transition to the next time-step.'''\n",
    "        if done == True: \n",
    "            break\n",
    "            \n",
    "\n",
    "    # Exploration rate decay\n",
    "    '''Once an episode is finished, you need to update your exploration_rate using exponential decay, \n",
    "    which just means that the exploration rate decays at a rate proportional to its current value. \n",
    "    You can decay the exploration_rate using the formula above, which makes use of all the exploration \n",
    "    rate parameters that were defined above in the hyperparameter section.'''\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    # Add current episode reward to total rewards list and move on to the next episode\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "  rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "  count = 1000\n",
    "\n",
    "  print(\"********Average reward per thousand episodes********\\n\")\n",
    "  for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000 \n",
    "  print(\"\\n\\n********Final Q-table********\\n\")\n",
    "  print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0NINgJd1je2S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0NINgJd1je2S",
    "outputId": "5379ef82-a0d4-44a2-d019-ea12ed404601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.2650000000000002\n",
      "2000 :  0.6960000000000005\n",
      "3000 :  0.7200000000000005\n",
      "4000 :  0.7300000000000005\n",
      "5000 :  0.7320000000000005\n",
      "6000 :  0.7160000000000005\n",
      "7000 :  0.6930000000000005\n",
      "8000 :  0.7310000000000005\n",
      "9000 :  0.7110000000000005\n",
      "10000 :  0.7100000000000005\n",
      "11000 :  0.7110000000000005\n",
      "12000 :  0.7020000000000005\n",
      "13000 :  0.7430000000000005\n",
      "14000 :  0.7110000000000005\n",
      "15000 :  0.7270000000000005\n",
      "16000 :  0.7170000000000005\n",
      "17000 :  0.7270000000000005\n",
      "18000 :  0.7080000000000005\n",
      "19000 :  0.7040000000000005\n",
      "20000 :  0.7430000000000005\n",
      "21000 :  0.7230000000000005\n",
      "22000 :  0.7140000000000005\n",
      "23000 :  0.7160000000000005\n",
      "24000 :  0.7490000000000006\n",
      "25000 :  0.7160000000000005\n",
      "26000 :  0.7120000000000005\n",
      "27000 :  0.7100000000000005\n",
      "28000 :  0.7160000000000005\n",
      "29000 :  0.7260000000000005\n",
      "30000 :  0.6990000000000005\n",
      "31000 :  0.7230000000000005\n",
      "32000 :  0.7130000000000005\n",
      "33000 :  0.7400000000000005\n",
      "34000 :  0.7070000000000005\n",
      "35000 :  0.7140000000000005\n",
      "36000 :  0.7390000000000005\n",
      "37000 :  0.7070000000000005\n",
      "38000 :  0.7010000000000005\n",
      "39000 :  0.7440000000000005\n",
      "40000 :  0.7090000000000005\n",
      "41000 :  0.7020000000000005\n",
      "42000 :  0.7180000000000005\n",
      "43000 :  0.7280000000000005\n",
      "44000 :  0.7030000000000005\n",
      "45000 :  0.7260000000000005\n",
      "46000 :  0.7000000000000005\n",
      "47000 :  0.7150000000000005\n",
      "48000 :  0.7250000000000005\n",
      "49000 :  0.7270000000000005\n",
      "50000 :  0.7480000000000006\n",
      "\n",
      "\n",
      "********Final Q-table********\n",
      "\n",
      "[[0.530165   0.42923632 0.4318452  0.43291408]\n",
      " [0.23795439 0.22139874 0.15667533 0.4187216 ]\n",
      " [0.32107746 0.0905391  0.18437266 0.18571856]\n",
      " [0.08218495 0.         0.         0.        ]\n",
      " [0.55809774 0.40202285 0.36017507 0.4095842 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.32776027 0.04169591 0.04761828 0.04384778]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.26113258 0.29326804 0.38956035 0.61425146]\n",
      " [0.39635966 0.68129123 0.38927978 0.21468599]\n",
      " [0.65698516 0.29108754 0.21853942 0.25341284]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.44368975 0.44515439 0.8525689  0.52970942]\n",
      " [0.62626514 0.90572305 0.70530461 0.6123143 ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#num_episodes, max_steps_per_episode, learning_rate, discount_rate, exploration_rate, max_exploration_rate, min_exploration_rate, exploration_decay_rate\n",
    "run_experiment(50000, 500, 0.2, 0.99, 1, 1, 0.001, 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mT3AdzIF4dT1",
   "metadata": {
    "id": "mT3AdzIF4dT1"
   },
   "source": [
    "### Testplan to optimize the algorithm:\n",
    "\n",
    "We are working with the Q-algorithm to train our agent to reach the frisbee and learn from its mistakes. To calculate the new Q-value we use the formula:\n",
    "\n",
    "$q^{new}(s,a) = (1-\\alpha)q(s,a)+\\alpha[R_{t+1}+\\gamma max q (s', a')]$\n",
    "\n",
    "with: \n",
    "\n",
    "$\\alpha$ our learning rate, $\\gamma$ our discount rate and $R_{t+1}$ being the reward.\n",
    "\n",
    "\n",
    "Influence of variables (according to wikipedia)\n",
    "\n",
    "Learning rate $\\alpha$:\n",
    "- Determines to what extent newly acquired information overrides the old one.\n",
    "- 0 learns nothing (only using prior knowledge)\n",
    "- 1 makes the agent only consider the most recent information (ignoring prior knowledge to explore possibilities)\n",
    "- Fully deterministing -> alpha of 1 is optimal\n",
    "- Stochastic -> algorithm converges so it decreases to zero\n",
    "- In practice, a constant LR is used such as 0.1\n",
    "\n",
    "Discount factor $\\gamma$\n",
    "- Determines the importance of future rewards\n",
    "- Factor of 0 makes the agent \"myopic\" only short sighted and not considerent long term rewards\n",
    "- Factor of >= 1 makes the rewards become infinite\n",
    "\n",
    "Open Questions:\n",
    "- Why does a reward of 0 happen?\n",
    "\n",
    "Working out good Hyperparameters:\n",
    "\n",
    "First, differences in the learning rate and the discount rate are going to be observed. Personally, I think the discount rate should not be lowered too much, since we do not want to have an short sighted agent. The learning rate on the other side could be important.\n",
    "\n",
    "### Test 1: Base Hyperparameter\n",
    "\n",
    "num_episodes = 10000, max_steps_per_episode = 100, learning_rate = 0.1, discount_rate = 0.99, exploration_rate = 1, max_exploration_rate = 1, min_exploration_rate = 0.01, exploration_decay_rate = 0.01\n",
    "\n",
    "**Highest 1000 episodes value: 59%**\n",
    "\n",
    "Interestingly, the first run was all 0s, after restarting the kernel and run all, a increasing reward percentage could be observed.\n",
    "\n",
    "### Test 2: Increasing learning rate\n",
    "\n",
    "num_episodes = 10000, max_steps_per_episode = 100, learning_rate = 0.2,\n",
    "discount_rate = 0.99, exploration_rate = 1, max_exploration_rate = 1, min_exploration_rate = 0.01, exploration_decay_rate = 0.01\n",
    "\n",
    "**Highest 1000 episodes value: 68.7%**\n",
    "\n",
    "Because of the higher learning rate, the agent directly started with 51% in the first 1000 episodes. We can observe, that an higher learning rate led to generally higher average reward values.\n",
    "\n",
    "### Test 3: Increasing learning rate\n",
    "\n",
    "learning_rate = 0.3, discount_rate = 0.99, exploration_rate = 1, max_exploration_rate = 1, min_exploration_rate = 0.01, exploration_decay_rate = 0.01\n",
    "\n",
    "**Highest 1000 episodes value: 68.7%**\n",
    "\n",
    "The agent did not better his max average percentage. Overall all values converged towards 60-70 percent. However, a higher learning curve does not seem to help further improvement.\n",
    "\n",
    "### Test 4: Increased learning rate, lowered discount rate\n",
    "\n",
    "learning_rate = 0.2, discount_rate = 0.9, exploration_rate = 1, max_exploration_rate = 1, min_exploration_rate = 0.01, exploration_decay_rate = 0.01\n",
    "\n",
    "**Highest 1000 episodes value: 52%**\n",
    "\n",
    "We can observe, that lowering the discount rate does not help the algorithm, which was expected since we dont want short sighted agents. A increased learning rate, high discount rate and lower exploration decay could be interesting.\n",
    "\n",
    "### Test 5: Increased learning rate, same discount rate, lower exploratory decay\n",
    "\n",
    "learning_rate = 0.2, discount_rate = 0.99 , exploration_rate = 1, max_exploration_rate = 1, min_exploration_rate = 0.01, exploration_decay_rate = 0.005\n",
    "\n",
    "**Highest 1000 episodes value: 68%**\n",
    "\n",
    "The outcome is quite similar to Test 2 and 3, maybe we have to have to change up the min_exploration rate in order to keep the agent trying out longer.\n",
    "\n",
    "### Test 6: Increased learning rate, ame discount rate, lower exploratory decay, lower min_exploration rate\n",
    "\n",
    "num_episodes = 10000, max_steps_per_episode = 100, learning_rate = 0.2,  discount_rate = 0.99, exploration_rate = 1, max_exploration_rate = 1, min_exploration_rate = 0.001, exploration_decay_rate = 0.005\n",
    "\n",
    "**Highest 1000 episodes value: 74%**\n",
    "\n",
    "With this setup, we could break through the 70% mark. Possibly lower min exploration rates and lower exploration decays are key.\n",
    "\n",
    "\n",
    "\n",
    "## Final observations:\n",
    "- Adding more steps to the episodes did not help. \n",
    "- Changing the exploration_rate from 1 to 0.9 did not help\n",
    "- Adding 100000 episodes did not help\n",
    "- Generally a learning rate of 0.2 was optimal for our approach, higher ones resulted in worse results\n",
    "- Changing the $\\epsilon$-greedy parameters helped the agent\n",
    "  - lower exploration decay rate and lowe min exploration rate helped out -> more exploiting\n",
    "- 74-76% as barrier was not surpassed by trying out other hyperparameters."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
